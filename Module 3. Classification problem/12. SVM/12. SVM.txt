Find some lines that separate red beans and mung beans
central axis

The essence of SVM is: finding the optimal solution through mathematical methods

Hyperplane? :
In two dimensions it is a straight line, in three dimensions it is a plane, and in more than three dimensions it is a hyperplane.

What are support vectors?
Suppose you find a line that can separate red beans and mung beans
The few sample points closest to this line in red beans and mung beans are called support vectors.
The distance from these points to this line is called the interval
Only support vectors play a role in determining the optimal hyperplane, and other data points do not.
##This is also the origin of the name of support vector machine

How to deal with unclear boundaries?
Soft interval: A certain number of samples are allowed to appear in this interval area, which is called soft interval.
Otherwise, it is called a hard interval (no beans are allowed to appear in the interval)

How to deal with nonlinear separability?
Linear inseparability often occurs in data sets. For example, dry beans in a circle around a square. This cannot be divided by a line.
The method used in SVM is:
Map indivisible samples into high-dimensional space
## For example, in a three-dimensional space, a plane can be used to divide these beans.

In SVM, the "kernel function" is used to implement the operation of mapping to high dimensions. Common kernel functions include: linear kernel function, polynomial kernel function, Gaussian kernel function, etc.
## Reduce the amount of calculation and memory used for calculation

advantage:
It gives the global optimal solution (unlike the decision tree, which only gives the local optimal solution). It is supported by strict mathematical theory and has strong interpretability.
The algorithm is very robust. Because it mainly depends on the support vector, as long as the support vector does not change, changes in the sample set will have no impact on the division.

shortcoming:
The resources required for training are large (only suitable for a few thousand pieces of data)
Can only handle two-classification problems (combination methods are required to handle multi-classification problems)
Model prediction is that the prediction time is proportional to the number of support vectors (cannot handle big data)
找到一些线可以分隔红豆和绿豆
中轴线

SVM本质是：通过数学方法寻找最优解

超平面？：
二维中就是直线，三维中就是平面，而在三维以上时，就是超平面。

什么是支持向量？
假设找到一条线可以分割红豆和绿豆
红豆和绿豆中距离这条线最近的几个样本点被称为支持向量（support vector）
这些点到这条线的距离称为间隔
在决定最佳超平面时只有支持向量起作用，而其他的数据点并不起作用。
##这也是支持向量机名字的来源

如何处理不清晰的边界？
软间隔：在这个间隔区域里允许出现一定数量的样本，称为软间隔
反之，则称为硬间隔（不允许间隔中出现任何豆子）

如何处理非线性可分呢？
数据集中经常出现线性不可分的情况。比如方形的周围晒一圈豆子。这就无法用一条线来划分了。
SVM中采用的方法是：
把不可划分的样本映射到高维空间中
## 如在三维空间中，就可以用一个平面来划分这些豆子。

在SVM中借助“核函数”来实现映射到高维的操作。常见的核函数有：线性核函数、多项式核函数、高斯核函数，等
## 减小计算量和计算用的内存

优点：
给出了全局最优解（不像决策树，只给局部最优解），有严格的数学理论支持，可解释性强。
算法的鲁棒性很好。因为主要依赖于支持向量，所以只要支持向量没什么变化，样本集发生变化对划分没有什么影响

缺点：
训练所需的资源很大（只适合几千条数据）
只能处理二分类问题（需要使用组合手段才能处理多分类问题）
模型预测是，预测时间与支持向量的个数成正比（不能处理大数据）
