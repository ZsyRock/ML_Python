Assume there are M pieces of data in total, and the plan is divided into 3 categories

1. First randomly select 3 points in a data space, called center points
2. Calculate the distance from all points to these three points. The distance here is calculated as the Euclidean distance.
3. Use the data of each group to calculate a mean of these data, and use this mean as the center point of the next iteration.
. .
Repeat the above process to iterate
When the center point changes very little, the operation can be stopped.

the problem we are facing:
1. How to face the K value (I don’t know how many K there are in actual application)
Method: Elbow method: try the K value in a loop and calculate the loss of all data under different K values. That is, the average distance is calculated using the sum of the distances from each data point to the center point. When K is 1, the distance is maximum, and when M, it is 0. So in the process of increasing, you can see an inflection point, which is the elbow.
This method is only applicable when the K value is not too large. If the K value is tens of thousands, the learning rate must be set larger.
Overall, it is more time-consuming and labor-intensive

advantage:
Simple and clear. Low computational complexity
The convergence speed is fast, and the effect is generally better after several iterations.

shortcoming:
The result is unstable
Unable to approach sample imbalance problem
Easily converges to local optimal solution
Largely affected by noise
假设数据共M条，计划分为3类

1. 先随机在一个数据空间中选取3个点，称之为中心点
2. 计算所有的点到这三个点的距离，这里的距离计算的是欧氏距离
3. 使用每个组的数据计算出这些数据的一个均值，使用这个均值作为下一轮迭代的中心点。
。。
重复上面的过程进行迭代
直到中心点的变动很小时，就可以停止运行了

面临的问题：
1. 如何面临K值（实际运用是也不知道有几个K）
方法：手肘法：循环尝试K值，计算在不同的K值情况下，所有数据的损失。即用每一个数据点到中心点的距离之和计算平均距离。当K为1时距离最大，当M时为0.所以在加大的过程中可以看到一个拐点，也即是手肘。
这个方法只适用于K值不太大的时候，如果K值几千几万，就要设置学习率大一些。
总体而言比较费时费力

优点：
简洁明了.计算复杂度低
收敛速度较快，迭代数次后效果一般较好

缺点：
结果不稳定
无法接近样本不均衡问题
容易收敛到局部最优解
受噪声影响较大 


衍生方法：

K-means++：在选取中心点时进行优化。
从已有的数据中随机的进行多次选取K个中心点，每次都计算这一次选中的中心点的距离，然后去一组最大的作为初始化中心点。

mini batch K-means: 改进了数据量/维度变大时运算缓慢的问题。在迭代时，每个集合中选取一部分进行计算，从而降低计算的复杂度。

总结：
聚类与分类的区别
K-means算法，非常简洁的基于划分的聚类算法
