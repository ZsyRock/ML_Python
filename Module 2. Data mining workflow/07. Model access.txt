How to evaluate whether the model has reached the standard? Commonly used evaluation indicators:
1. Accuracy related indicators - (for example, 800 are pigs and 200 are not, you can draw a confusion matrix), which can directly reflect the learning situation of a model on sample data. It is a standardized test.
High positive (true positive TP)
true negative (TN)
False positive (FP)
False negative (FN)
Accuracy: (TP+TN)/(TP+FP+FN+TN)
Precision: the probability that the result of predicting TP accounts for all P: TP/(TP+FP)
Recall: the probability that the result of predicting TP accounts for all actual T: TP/(TP+FN)

ROC and AUC: (how many probability values to choose to determine the result)
Specify "yes" when the probability of "yes" is 0.1 or more
The probability of "yes" is "no" if it is less than 0.1
This results in multiple sets of confusion matrices.

Then each set of confusion matrices has:
True case rate: TP/(TP+FN)
False positive rate: FP/(FP+TN)
Use these two values to plot points in the coordinate system:
y = TP/(TP+FN)
x = FP/(FP+TN)
The ROC curve can be obtained, and the area under the curve is the AUC value. It can reflect the stability of a model. When the ROC is close to the diagonal line, it indicates that the model is very unstable.

2. Business sampling assessment
Since errors are inevitable in the modeling process and the model is specified based on the business, sampling evaluation can mitigate this situation.
3. Generalization ability assessment
##The model’s ability to judge unknown tasks
It can be judged by overfitting and underfitting.
One is learned to death (as long as it is different from the sample, it is wrong), the other is not learned (feature learning is incomplete)
At this time, it is necessary to reorganize the data, summarize the reasons for overfitting and underfitting, and then adjust the data and retrain.

Other evaluation indicators:
1. Model speed (overhead)
2. Robustness: Will incorrect data cause the model to collapse?
3. Explainability: In many scenarios (such as financial risk control), a convincing reason needs to be given


Processing of evaluation data:
Random sampling: Divide into training set and test machine, use the test machine to test the model, and obtain various accuracy indicators
Then take multiple samples: N groups of test machines are sampled, and the average value of the N groups of test machines is used as the final result.
Cross-validation: Same as the cross-validation method of 05: first divide the data set into N, use N-1 data sets for training each time, and the rest are used as test sets. Carry out N times of training with this.
Bootstrapping method: Same as 05 bootstrapping method: when the data set is small, the data set is constructed through repeated sampling. Randomly extract samples with replacement to construct a training set, and train K models. The results are used as the accuracy obtained for this question.

如何评估模型是否已经达标？常用的评估指标：
1. 准确率相关指标-（如800只是猪，200只不是，可以画一个混淆矩阵出来）可以直接反应一个模型对于样本数据的学习情况。是一种标准化的检验。
高阳性（true positive TP）
真阴性（TN）
假阳性（FP）
假阴性（FN）
准确率Accuracy：（TP+TN）/(TP+FP+FN+TN)
精确率（precision）：预测TP的结果占所有P的概率：TP/(TP+FP)
召回率（Recall）:预测TP的结果占所有实际T的概率：TP/(TP+FN)

ROC和AUC：（选多少概率值来判定结果）
指定”是“的概率为0.1以上时为”是“
”是“的概率为0.1以下为”否“
以此得出多组混淆矩阵。

那么每一组混淆矩阵都有：
真正例率：TP/(TP+FN)
假正例率：FP/(FP+TN)
使用这两个值在坐标系中描点：
y = TP/(TP+FN)
x = FP/(FP+TN)
即可得出ROC曲线，曲线下的面积为AUC值。可以反应一个模型的稳定性。当ROC接近对角线时，说明模型很不稳定。

2. 业务抽样评估
由于建模过程难免有错误产生，模型是基于业务指定的，进行抽样评估可以减弱这种情况
3. 泛化能力评估
##模型对与未知任务的判断能力
可以通过overfitting和underfitting来判断。
一个是学死了（只要和样本不同就不对），一个是没学会（特征学习不完全）
此时需要对数据进行重新整理，总结出过拟合和欠拟合的原因，然后调整数据重新进行训练。

其他评估指标：
1. 模型速度（开销）
2. 鲁棒性：错误数据是否会导致模型崩溃
3. 可解释性：在很多场景下（比如金融风控），需要给出一个让人信服的理由


评估数据的处理：
随机抽样：分成训练集和测试机，使用测试机对模型进行测试，得到各种准确率指标
随即多次抽样：抽样得到N组测试机，使用N组的测试机的平均值作为最终结果
交叉验证：同05的交叉验证法：先把数据集划分为N个，每次使用N-1个数据集训练，剩下的作为测试集。以此进行N次训练。
自助法：同05自助法：小数据集时，通过重复抽样构建数据集。随机有放回抽取样本构建训练集，训练K个模型结果作为这个题获得的准确率
