在已知条件中，选一个条件作为树根，然后再看是否还需要其他判断条件。
如果需要其他条件，就继续构建分支。。。

最终形成的这棵树上，所有的叶子节点都是要输出的类别信息
所有的非叶子节点都是特征信息

当一个数据来了之后，按照条件就可以知道结果

问题：
1. 该如何选择一个特征作为根节点？
2. 下一次决策又该选择哪个特征作为节点？

决策树算法使用信息增益的方法来衡量一个特征和特征之间的重要性。
当信息 增益越大，表明这个特征越重要，那么优先对这个特征进行决策。

理想情况下，每一个叶子节点都是一个纯粹的分类
实际情况下，都采用了贪心算法，即都是局部最优解，而不是全局最优解，

决策树的发展：ID3->C4.5->CART(classification and regression tree)，最广泛使用的CART决策树（sklearn中使用的也是这种决策树），模型类型为分类与回归，树结构为二叉树，特征选择是基尼系数，可以处理连续纸，缺失值，也可以进行剪枝处理。

优点：
1. 非常直观，以图形化的方式展现出来
2 速度快，树形结构有助于提升运算速度判定很快
3. 可以处理离散连续缺失值

缺点：
1。 容易过拟合（出现问题数据容易泛化性能不好）
2. 需要处理样本不均衡（处理不好会倾向于一个数量过大的特征）
3. 样本的变化会导致树结构巨变

关于剪枝：
预剪枝：先设置阈值。但这种不常用
后剪枝：在决策树构建完成后，再根据设定的条件来判断是否要合并中间节点用叶子节点代替


随机森林：
使用bagging方案构建了多棵决策树，然后对所有树的结果来进行平均计算以获得最终的结果

GBDT:
GBDT构建的多棵树之间是有联系的，每个分类器在上一轮分类器的残差基础上进行训练

XGBoost：
- 机器学习大杀器，很多比赛中都获得良好结果
优化了GBDT里面的求解过程，并加入了很多工程上的优化项目
##实际上XGBoost也不能算一种算法。
